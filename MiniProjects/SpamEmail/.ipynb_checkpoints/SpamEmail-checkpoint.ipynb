{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project: Spam Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 1s 12ms/step - loss: 0.7281 - accuracy: 0.5075 - val_loss: 0.7111 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7102 - accuracy: 0.5163 - val_loss: 0.7088 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7018 - accuracy: 0.5250 - val_loss: 0.6991 - val_accuracy: 0.5050\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6999 - accuracy: 0.5400 - val_loss: 0.7156 - val_accuracy: 0.4850\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6958 - accuracy: 0.5150 - val_loss: 0.7074 - val_accuracy: 0.5150\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5487 - val_loss: 0.6948 - val_accuracy: 0.5250\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5700 - val_loss: 0.7146 - val_accuracy: 0.5050\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5550 - val_loss: 0.7047 - val_accuracy: 0.5050\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.5425 - val_loss: 0.7101 - val_accuracy: 0.5100\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.5625 - val_loss: 0.7077 - val_accuracy: 0.5000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7077 - accuracy: 0.5000\n",
      "Loss: 0.7077093720436096, Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the synthetic data\n",
    "data = pd.read_csv('synthetic_spam_data.csv')\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = data.drop('label', axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Adding layers to the model\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(100,)))  # Assume 100 features for each email\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Output layer to predict spam or not\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation \n",
    "\n",
    "1. **Importing Necessary Libraries**:\n",
    "   - `pandas` for data manipulation.\n",
    "   - `tensorflow` for building and training the neural network.\n",
    "   - `sklearn.model_selection` for splitting the dataset into training and testing sets.\n",
    "\n",
    "2. **Loading Data**:\n",
    "   - The synthetic spam dataset is loaded from a CSV file into a pandas DataFrame.\n",
    "\n",
    "3. **Preparing Data**:\n",
    "   - The data is split into features (`X`) and labels (`y`).\n",
    "   - It's then further split into training and testing sets, with 80% of the data used for training and 20% for testing.\n",
    "   \n",
    "       The line `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)` is using the `train_test_split` function from `scikit-learn` to split the dataset into training and testing sets.\n",
    "\n",
    "        1. **Function Call**: `train_test_split` is a function from the `sklearn.model_selection` module.\n",
    "   \n",
    "        2. **Input Arguments**:\n",
    "           - `X` and `y` are the features and labels of the dataset, respectively.\n",
    "           - `test_size=0.2` specifies that 20% of the data should be allocated to the testing set, while the remaining 80% goes to the training set.\n",
    "           - `random_state=42` is a seed for the random number generator, ensuring that the split is reproducible. This means that every time this line is run, the same split will be generated, which is important for consistency and comparing models.\n",
    "\n",
    "        3. **Output**:\n",
    "           - `X_train` and `y_train` are the features and labels for the training set, respectively.\n",
    "           - `X_test` and `y_test` are the features and labels for the testing set, respectively.\n",
    "\n",
    "        4. **Operation**:\n",
    "           - The function shuffles the dataset (using the specified random seed) and then splits it into training and testing sets according to the specified test size ratio. This is crucial for training robust models, as it ensures that the model is evaluated on data it hasn't seen during training.\n",
    "\n",
    "\n",
    "4. **Building the Model**:\n",
    "   - A Sequential model is initialized.\n",
    "   - Two layers are added: a dense layer with 16 neurons and ReLU activation, followed by a dense output layer with 1 neuron and sigmoid activation. This architecture is simple yet suitable for a binary classification task like spam detection.\n",
    "\n",
    "5. **Compiling the Model**:\n",
    "   - The model is compiled using the Adam optimizer, binary crossentropy loss (which is common for binary classification tasks), and accuracy as the evaluation metric.\n",
    "\n",
    "6. **Training the Model**:\n",
    "   - The model is trained for 10 epochs with a batch size of 32 using the training data, and validation data is provided for evaluating the model after each epoch.\n",
    "\n",
    "7. **Evaluating the Model**:\n",
    "   - The model is evaluated on the testing set, and the loss and accuracy are printed out.\n",
    "\n",
    "**Output Explanation**:\n",
    "\n",
    "The output shows the progress of training across 10 epochs. For each epoch, you see the following:\n",
    "- Training loss (`loss`) and accuracy (`accuracy`) on the training data.\n",
    "- Validation loss (`val_loss`) and accuracy (`val_accuracy`) on the validation data (which in this case is the testing set).\n",
    "\n",
    "The goal during training is to minimize the loss while maximizing the accuracy. However, the training and validation metrics suggest that the model is struggling to learn from the data. Specifically:\n",
    "- The training accuracy starts at around 51% and increases slightly to about 56%, while the validation accuracy hovers around 50% throughout. \n",
    "- The losses decrease slightly but not significantly, and the validation loss is even increasing at some epochs.\n",
    "\n",
    "The final evaluation on the test set shows a loss of about 0.708 and an accuracy of 50%. This 50% accuracy is no better than random guessing in a binary classification task, suggesting that the model hasn't learned to distinguish spam from non-spam emails effectively from the synthetic data provided.\n",
    "\n",
    "This could be due to several factors including the simplicity of the model, the quality or representativeness of the synthetic data, or the need for further preprocessing or feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
